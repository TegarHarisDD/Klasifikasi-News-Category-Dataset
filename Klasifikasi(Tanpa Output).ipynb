{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f40629",
   "metadata": {},
   "source": [
    "# KLASIFIKASI \"NEWS CATEGORY DATASET\" (KAGGLE)\n",
    "\n",
    "https://www.kaggle.com/datasets/rmisra/news-category-dataset\n",
    "\n",
    "Tegar Haris DD - A11.2022.14428 - PBA-4602"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae002b09",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "\n",
    "This dataset contains around 210k news headlines from 2012 to 2022 from HuffPost. This is one of the biggest news datasets and can serve as a benchmark for a variety of computational linguistic tasks. HuffPost stopped maintaining an extensive archive of news articles sometime after this dataset was first collected in 2018, so it is not possible to collect such a dataset in the present day. Due to changes in the website, there are about 200k headlines between 2012 and May 2018 and 10k headlines between May 2018 and 2022.\n",
    "\n",
    "Content\n",
    "Each record in the dataset consists of the following attributes:\n",
    "\n",
    "category: category in which the article was published.\n",
    "headline: the headline of the news article.\n",
    "authors: list of authors who contributed to the article.\n",
    "link: link to the original news article.\n",
    "short_description: Abstract of the news article.\n",
    "date: publication date of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3024afed",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0bb327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1684aa",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Karena seluruh dataset sangat besar kita load 5000 record untuk 5 kategori berita yang kita pilih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "\n",
    "# Pilih 5 kategori\n",
    "categories = ['POLITICS', 'ENTERTAINMENT', 'SPORTS', 'STYLE & BEAUTY', 'BUSINESS']\n",
    "\n",
    "# Filter data\n",
    "df = df[df['category'].isin(categories)]\n",
    "\n",
    "# Sampling 5000 per kategori\n",
    "sampled_dfs = []\n",
    "for category in categories:\n",
    "    category_df = df[df['category'] == category]\n",
    "    if len(category_df) > 5000:\n",
    "        category_df = category_df.sample(5000, random_state=42)\n",
    "    sampled_dfs.append(category_df)\n",
    "\n",
    "df = pd.concat(sampled_dfs)\n",
    "df['text'] = df['headline'] + \" \" + df['short_description']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a260d",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "Pembersihan teks dengan melakukan lowercasing, tokenisasi, stopword removal, dan stemming/lemmatization.\n",
    "\n",
    "Visualisasikan distribusi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Stopword removal and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebd741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualisasi\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.countplot(data=df, x='category', order=df['category'].value_counts().index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribusi Kategori Berita')\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "df['text_length'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['text_length'], bins=50)\n",
    "plt.title('Distribusi Panjang Teks')\n",
    "plt.xlabel('Jumlah Kata')\n",
    "plt.savefig('text_length_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Split Data\n",
    "X = df['cleaned_text']\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9043a6",
   "metadata": {},
   "source": [
    "## Ekstraksi Fitur, Pembuatan Model, Training, dan Evaluasi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db8d50",
   "metadata": {},
   "source": [
    "### Ekstraksi Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstraksi Fitur\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84b49d",
   "metadata": {},
   "source": [
    "### Pembuatan Model SVM, Training dan Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef941ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dengan Grid Search\n",
    "parameters = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "clf = GridSearchCV(svc, parameters, cv=3, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afa227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi\n",
    "best_svc = clf.best_estimator_\n",
    "y_pred_svm = best_svc.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nSVM Best Parameters:\", clf.best_params_)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_svc.classes_, \n",
    "            yticklabels=best_svc.classes_)\n",
    "plt.title('Confusion Matrix - SVM')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('svm_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728646f",
   "metadata": {},
   "source": [
    "### Pembuatan Model Logistic Regression, Training dan Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Training dengan Grid Search untuk Logistic Regression\n",
    "logreg_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression(random_state=42, multi_class='auto')\n",
    "logreg_clf = GridSearchCV(logreg, logreg_params, cv=3, n_jobs=-1, verbose=1)\n",
    "logreg_clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced30cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi\n",
    "best_logreg = logreg_clf.best_estimator_\n",
    "y_pred_logreg = best_logreg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nLogistic Regression Best Parameters:\", logreg_clf.best_params_)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm_logreg = confusion_matrix(y_test, y_pred_logreg)\n",
    "sns.heatmap(cm_logreg, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=best_logreg.classes_, \n",
    "            yticklabels=best_logreg.classes_)\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('logreg_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef055b34",
   "metadata": {},
   "source": [
    "### Pembuatan Dataframe, Model DistilBERT, Training dan Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization & Dataset Preparation\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, label2id):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = [label2id[label] for label in labels]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Label encoding\n",
    "label2id = {cat: i for i, cat in enumerate(categories)}\n",
    "id2label = {i: cat for cat, i in label2id.items()}\n",
    "\n",
    "train_dataset = NewsDataset(X_train, y_train, label2id)\n",
    "test_dataset = NewsDataset(X_test, y_test, label2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Preparation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(categories)\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop \n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    loop = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b59222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi \n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=categories))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm_bert = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Purples',\n",
    "            xticklabels=categories,\n",
    "            yticklabels=categories)\n",
    "plt.title('Confusion Matrix - DistilBERT')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('distilbert_confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00209b",
   "metadata": {},
   "source": [
    "## Evaluasi Seluruh Model dan Kesimpulan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a23e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandingkan akurasi semua model \n",
    "accuracies = {\n",
    "    'SVM': accuracy_score(y_test, y_pred_svm),\n",
    "    'Logistic Regression': accuracy_score(y_test, y_pred_logreg),\n",
    "    'DistilBERT': accuracy_score(all_labels, all_preds)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette='viridis')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Perbandingan Akurasi Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1721d",
   "metadata": {},
   "source": [
    "### Kesimpulan\n",
    "\n",
    "Berdasarkan hasil evaluasi pada tiga model klasifikasi teks untuk kategori berita HuffPost:\n",
    "\n",
    "- **SVM** menghasilkan akurasi: 86% (Dengan waktu training 6 Menit 34 detik)\n",
    "- **Logistic Regression** menghasilkan akurasi: 86% (Dengan waktu training 3.8 detik)\n",
    "- **DistilBERT** menghasilkan akurasi tertinggi: 89% (Dengan waktu training 22 menit 9 detik)\n",
    "\n",
    "Hal ini menunjukkan bahwa model berbasis transformer (DistilBERT) mampu memahami konteks dan makna teks lebih baik dibandingkan model klasik berbasis fitur (SVM dan Logistic Regression),sehingga memberikan performa klasifikasi yang lebih baik pada data headline dan deskripsi singkat berita.\n",
    "\n",
    "Namun, model klasik seperti SVM dan Logistic Regression tetap memberikan hasil yang sangat kompetitif dengan waktu pelatihan yang jauh lebih singkat dan kebutuhan komputasi yang lebih rendah, terlebih lagi Logistic Regression yang jauh sangat cepat dibanding svm walaupun dengan akurasi yang sama. Pemilihan model terbaik dapat disesuaikan dengan kebutuhan, ketersediaan sumber daya, dan kompleksitas data.\n",
    "\n",
    "Secara keseluruhan, seluruh model sudah cukup baik dalam mengklasifikasikan berita ke dalam lima kategori utama."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
